# llm-from-scratch
Code to write, train and fine-tune "famous" LLMs from scratch in Pytorch.

## Load a model
```
# Load GPT2-models: "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"
from src.load_gpt2 import GPT2_model
model = GPT2_model("gpt2").to(device)

# Load pretrained version
model = GPT2_model.from_pretrained("gpt2").to(device)
```

## Instruction Fine-tune on Alpaca GPT-4 dataset
The [Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca/tree/main) is a 52K instruction-following synthetic dataset developed by Stanford researchers using the OpenAI davinci model. The [AlpaGasus dataset](https://lichang-chen.github.io/AlpaGasus/) is a filtered version of this dataset containing only 9k high-quality data that significantly outperforms the original Alpaca as evaluated by GPT-4. We will be using this dataset. 
```
wget https://raw.githubusercontent.com/gpt4life/alpagasus/main/data/filtered/chatgpt_9k.json
```
The dataset is just a single JSON file, that contains prompts in Alpaca style:
 
```
instruction: str, describes the task the model should perform. 
                  Each of the 52K instructions is unique.
input:       str, optional context or input for the task.
output:      str, the answer to the instruction as generated by GPT-4.
```


* In order to fine-tune, run `main.py` file. 
* To chat with a fine-tuned model, run `chat.py` file. 

