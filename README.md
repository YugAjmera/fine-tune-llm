# Fine-tune LLMs from Scratch
Code to fine-tune "famous" LLMs from scratch in Pytorch.

## Load a model
Currently, only GPT-2 models are supported.
```
# Load GPT2-models: "gpt2", "gpt2-medium", "gpt2-large", "gpt2-xl"
from src.load_gpt2 import GPT2_model
model = GPT2_model("gpt2").to(device)

# Load pretrained version
model = GPT2_model.from_pretrained("gpt2").to(device)
```

## Instruction Fine-tuning on Alpaca or variants
You can choose from the following datasets:

1. The [Alpaca dataset](https://github.com/tatsu-lab/stanford_alpaca/tree/main) is a synthetic dataset developed by Stanford researchers using OpenAI's davinci model to generate 52k instruction/output pairs. 
```
wget https://raw.githubusercontent.com/tatsu-lab/stanford_alpaca/main/alpaca_data.json
```

2. The [Alpaca-GPT4 dataset](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM) uses GPT-4, instead of text-davinci-003 (GPT-3), to answer the same prompts. As a result, it contains higher-quality and longer responses.
```
wget https://raw.githubusercontent.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM/main/data/alpaca_gpt4_data.json
```

3. The [AlpaGasus dataset](https://lichang-chen.github.io/AlpaGasus/) is a filtered version of Alpaca, containing only 9k high-quality examples. It has been shown to significantly outperform models trained on the original Alpaca, as evaluated by GPT-4.
```
wget https://raw.githubusercontent.com/gpt4life/alpagasus/main/data/filtered/chatgpt_9k.json
```

### Alpaca style
All the datasets are just a single JSON file, containing prompts in Alpaca style:
```
instruction: str, describes the task the model should perform. 
                  Each of the 52K instructions is unique.
input:       str, optional context or input for the task.
output:      str, the answer to the instruction as generated by GPT-4.
```

### Scripts
* To fine-tune the model, run `main.py` file. 
* To chat with a fine-tuned model, run `chat.py` file. 

